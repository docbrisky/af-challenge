{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook, you will need to ensure your environment runs with at least Python 3.6 and has all the dependencies installed (see below). You will need to download a copy of the file \"ResNet_30s_34lay_16conv.hdf5\" from this GitHub respository: https://github.com/fernandoandreotti/cinc-challenge2017/tree/master/deeplearn-approach and store it in the same folder as this notebook. You will also need to download the file \"training2017.zip\" from this page: https://physionet.org/challenge/2017/ and unzip it into the same folder as this notebook (so that the .mat and .hea files are contained in a subdirectory named \"training2017\". Finally, you should create an empty subdirectory named \"training_nps\".\n",
    "\n",
    "Once all this is done, you should be able to hit \"Run All\". The experiment was originally run on a machine with an Intel i7 processor, a NVIDIA GeForce GTX 1060 GPU and 8GB RAM. With these specifications, it takes about 36 hours to execute in full. Approximately 7GB of HDD space will be required for the ECG images. \n",
    "\n",
    "The output of the code will be approximately 110,000 images of ECG traces and an automated analysis system that can classify them into one of four cardiac rhythm groups with an F1 score in excess of 0.8 (as evaluated by 5-fold cross-validation, where no version of the model has been exposed to any data from patients whose ECGs it is tasked with analysing during the validation process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "import scipy.io as sio\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import cv2\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from multiprocessing import Pool \n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "from keras.models import Sequential, Model, model_from_json, load_model\n",
    "from keras.layers import Input, Flatten, Dense, Conv2D, Activation, MaxPool2D,MaxPooling1D,Conv1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils, Sequence\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes as input a directory containing .mat files with the raw data from single lead ECG recordings.\n",
    "The signal data takes the forms of a list of readings in mV at a frequency specificied by the FS (frames/second) value.\n",
    "\n",
    "It transposes each of those records into a series of .png images where height = image_y and width = (image_y * ecg_seconds).\n",
    "\n",
    "It returns a list of filenames for the created images in the following format: 'temp_ecgs/[original_record_name]\\_[segment_number].png', where the original_record_name has had the '.mat' extesion removed. It also returns a list of the original record names along with a list of corresponding target labels.\n",
    "\n",
    "The function can take a while to run, so its outputs of the function are saved to disk for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ecg_images(image_y,ecg_seconds,FS, directory,sliding_window_seconds,max_seconds_of_padding,script_runs,records_per_batch):\n",
    "    \n",
    "    \n",
    "    #Ensure the target directory for the images exists:\n",
    "    dirName='temp_ecgs'\n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir(dirName)\n",
    "        print(\"Directory \" , dirName ,  \" created. Creating ECGs...\") \n",
    "    except FileExistsError:\n",
    "        print(\"Directory \" , dirName ,  \" exists. Continuing with next batch of ECG creation...\")\n",
    "    \n",
    "    \n",
    "    ecg_list=[] #Will be returned as a list of image file names for the ECGs of ecg_seconds long\n",
    "    image_x = image_y * ecg_seconds #Calculate the image width based on its height. Note actual image size on save may vary substantially, will be rescaled on import.\n",
    "    length=FS*ecg_seconds #Calculate how many values to include in each ECG segment (i.e. each image)\n",
    "    image_list=[] #List of images already created (in case function was interrupted before all records processed)\n",
    "    ground_truth=[] #List of labels\n",
    "    original_ecg_list=[] #List of original record names (without '.mat' extension). Only records >= ecg_seconds included.\n",
    "    ecgs_in_physionet=0 #Total number of records in source, for comparison with length of original_ecg_list (to see how many records were too short to be included).\n",
    "    sliding_window_frames=sliding_window_seconds*FS\n",
    "    max_frames_of_padding=max_seconds_of_padding*FS\n",
    "    df = pd.read_csv(directory + \"/REFERENCE.csv\") #Load up label database.\n",
    "    z=0 #Helper variable in case images need to be written in batches (can clog up memory of too many at once).\n",
    "    for file in glob.glob(\"temp_ecgs/*.png\"): #Loop to obtain list of images already created.\n",
    "        image_name=file.split('.')[0]\n",
    "        image_name=image_name.split('\\\\')[-1]\n",
    "        image_list.append(image_name)\n",
    "    for file in glob.glob(directory + \"/*.mat\"): #Main loop.\n",
    "        if (z<records_per_batch+(script_runs*records_per_batch)): #Helper function for writing images in batches. Always initialise z=0 - files already written will be ignored.\n",
    "            ecgs_in_physionet+=1 #Count how many records in source data.\n",
    "            matlab_array_contents= sio.loadmat(file)\n",
    "            ecg = matlab_array_contents['val'] #Load ECG values from '.mat' file.\n",
    "            padding=np.zeros((1,max_frames_of_padding))\n",
    "            ecg=np.concatenate((ecg,padding),axis=1)\n",
    "            maxi=np.argmax(ecg[0,:])\n",
    "            mini=np.argmin(ecg[0,:]) #Get minimum and maximum mV values for rescaling later.\n",
    "            del matlab_array_contents #Save on memory.\n",
    "            if(ecg.shape[1] >= length): #Ensure records is at least ecg_seconds long.\n",
    "                segments=int((ecg.shape[1] - length) // sliding_window_frames) #Calculate how many segments can be gleaned from each record.\n",
    "                #print('Length of ECG:',ecg.shape[1]/FS,'Segments:',segments)\n",
    "                for k in range (segments):\n",
    "                    savename = file.split('.')\n",
    "                    savename = savename[0].split('\\\\')[-1] #Isolate record name without directory prefix and '.mat' extension.\n",
    "                    ground_truth.append(df[\"label\"][df[\"ECG\"] == savename].values[0]) #Add corresponding label to ground_truth list.\n",
    "                    if (savename not in original_ecg_list):\n",
    "                        original_ecg_list.append(savename) #Add the record name to original_ecg_list\n",
    "                    savename=savename+'_'+str(k).zfill(3) #Now adjust the filename to '[record_name]_[segment_number]'.\n",
    "                    if (savename not in image_list): #Check image for this segment hasn't already been created.\n",
    "                        savename = './temp_ecgs/' + savename + '.png'\n",
    "                        ecg_list.append(savename) #Add image filename to ecg_list.\n",
    "                        ecg_k=ecg - np.mean(ecg)\n",
    "                        ecg_k=ecg_k/np.std(ecg_k)\n",
    "                        ecg_k=np.interp(ecg_k, (ecg_k.min(), ecg_k.max()), (0, +1)) #These three lines for feature scaling.\n",
    "                        ecg_k=ecg_k[0,k*sliding_window_frames:length+(k*sliding_window_frames)] #Select out segment from total list of ECGs values for this record.\n",
    "                        fig, ax = plt.subplots()\n",
    "                        ax.set(xlim=[0, length],ylim=[0,1])\n",
    "                        ax.plot(ecg_k,'k')\n",
    "                        DPI = fig.get_dpi()\n",
    "                        fig.set_size_inches(image_x/DPI,image_y/DPI)\n",
    "                        ax.set_axis_off()\n",
    "                        extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "                        fig.savefig(savename,facecolor='w',bbox_inches=extent)\n",
    "                        plt.close(fig) #These nine lines create, save and close the images using standard tools from MatPlotLib.\n",
    "                        del ax\n",
    "                        del fig #Save memory.\n",
    "                        del ecg_k\n",
    "                    else:\n",
    "                        savename = './temp_ecgs/' + savename + '.png'\n",
    "                        ecg_list.append(savename) #If the image has already been created, simply add its filename to list.\n",
    "            else:\n",
    "                #print('This ECG is less than 30s!')\n",
    "                k=0\n",
    "                savename = file.split('.')\n",
    "                savename = savename[0].split('\\\\')[-1] #Isolate record name without directory prefix and '.mat' extension.\n",
    "                ground_truth.append(df[\"label\"][df[\"ECG\"] == savename].values[0]) #Add corresponding label to ground_truth list.\n",
    "                if (savename not in original_ecg_list):\n",
    "                    original_ecg_list.append(savename) #Add the record name to original_ecg_list\n",
    "                savename=savename+'_'+str(k).zfill(4) #Now adjust the filename to '[record_name]_[segment_number]'.\n",
    "                if (savename not in image_list): #Check image for this segment hasn't already been created.\n",
    "                    savename = './temp_ecgs/' + savename + '.png'\n",
    "                    ecg_list.append(savename) #Add image filename to ecg_list.\n",
    "                    ecg_k=np.zeros(length)\n",
    "                    ecg_temp=ecg - np.mean(ecg)\n",
    "                    ecg_temp=ecg_temp/np.std(ecg_temp)\n",
    "                    ecg_temp=np.interp(ecg_temp, (ecg_temp.min(), ecg_temp.max()), (0, +1)) #These three lines for feature scaling.\n",
    "                    ecg_k[:ecg_temp.shape[1]]=ecg_temp[0,:] #Select out segment from total list of ECGs values for this record.\n",
    "                    del ecg_temp\n",
    "                    fig, ax = plt.subplots()\n",
    "                    ax.set(xlim=[0, length],ylim=[0,1])\n",
    "                    ax.plot(ecg_k,'k')\n",
    "                    DPI = fig.get_dpi()\n",
    "                    fig.set_size_inches(image_x/DPI,image_y/DPI)\n",
    "                    ax.set_axis_off()\n",
    "                    extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "                    fig.savefig(savename,facecolor='w',bbox_inches=extent)\n",
    "                    plt.close(fig) #These nine lines create, save and close the images using standard tools from MatPlotLib.\n",
    "                    del ax\n",
    "                    del fig #Save memory.\n",
    "                    del ecg_k\n",
    "                else:\n",
    "                    savename = './temp_ecgs/' + savename + '.png'\n",
    "                    ecg_list.append(savename) #If the image has already been created, simply add its filename to list.\n",
    "            del ecg\n",
    "            z+=1\n",
    "    print('Lists created! ECG_list length: ',len(ecg_list),' Truth list length: ',len(ground_truth))\n",
    "    print('ECGs in list: ', len(original_ecg_list),' ECGs provided by Physionet: ',ecgs_in_physionet)\n",
    "    with open('training_nps/ecg_list.p', 'wb') as list_file:\n",
    "            pickle.dump(ecg_list, list_file)\n",
    "    with open('training_nps/ground_truth.p', 'wb') as list_file:\n",
    "            pickle.dump(ground_truth, list_file)\n",
    "    with open('training_nps/original_ecg_list.p', 'wb') as list_file:\n",
    "            pickle.dump(original_ecg_list, list_file)\n",
    "    return [ecg_list,ground_truth,original_ecg_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below returns the same values as the previous function, if the values have already been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lists():\n",
    "    print(\"Loading pickled lists...\")\n",
    "    with open('training_nps/ecg_list.p', 'rb') as list_file:\n",
    "            ecg_list = pickle.load(list_file)\n",
    "    with open('training_nps/ground_truth.p', 'rb') as list_file:\n",
    "            ground_truth = pickle.load(list_file)\n",
    "    with open('training_nps/original_ecg_list.p', 'rb') as list_file:\n",
    "            original_ecg_list = pickle.load(list_file)\n",
    "    print(\"Done!\")\n",
    "    return [ecg_list,ground_truth,original_ecg_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below measures the mean, max and min mV values from the entire dataset. It can be useful if you want to rescale your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ecg_ranges(directory):\n",
    "    maxi=0\n",
    "    mini=0\n",
    "    maxis=0\n",
    "    minis=0\n",
    "    for file in glob.glob(directory + \"/*.mat\"):\n",
    "        matlab_array_contents= sio.loadmat(file)\n",
    "        ecg = matlab_array_contents['val'][0,:]\n",
    "        ecg = ecg - np.mean(ecg)\n",
    "        ecg = ecg/np.std(ecg)\n",
    "        maxi+=np.amax(ecg)\n",
    "        mini+=np.amin(ecg)\n",
    "        maxis+=1\n",
    "        minis+=1\n",
    "    ave_max=maxi/maxis\n",
    "    ave_min=mini/minis\n",
    "    print('Average y_max: ', ave_max, ' Average y_min: ', ave_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes an image array and loops through the columns looking for the row index of the first encountered black pixel. Once all row indices are logged, any missing values are filled in by averaging values from the row indicies in neighbouring columns. This data will later be treated as a surrogate for raw ECG data (i.e. in mV / [second / Hz]) in order to make use of previous work taking raw ECG data as input.\n",
    "\n",
    "In our paper, the ECGs were  transposed to image files at a resolution that only preserved half the data (approximating the information loss one would expect if using an old desktop scanner). For this reason, the function below subsequently expands the generated image_x-dimensional vector by a factor of two using more nearest-neighbour averaging. (We wanted to use a pre-trained model including the final, fully-connected layer. Thus, we needed to preserve the length of the input feature vector.)\n",
    "\n",
    "All values are rescaled between 0-1 using linear interpolation. The resulting 2(image_x)-dimensional vector forms the input for the ResNet convolutional neural network, whose architecture is based on recent research from Stanford University and the University of Oxford on rhythm recognition using deep learning.\n",
    "\n",
    "Currently this function relies on a very clean dataset, because any artefactual black pixels or rotation of the source image would skew the results. There are several ways this process could be made more robust. However, our intention in conducting this study was test the null hypothesis that deep learning based ECG analysis using image inputs would result in catastrophic loss of classifier performance. Refining the system will no doubt form the basis of future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manually_read_image(data):\n",
    "    try:\n",
    "        file=data[0]\n",
    "        img=data[1]\n",
    "        y_list=[]\n",
    "        for column in range(img.shape[1]):\n",
    "            switch = False\n",
    "            for row in range(img.shape[0]):\n",
    "                if (img[row,column]<0.2 and switch==False):\n",
    "                    if (row==0):\n",
    "                        y_list.append(1)\n",
    "                        switch=True\n",
    "                    else:\n",
    "                        y_list.append(1-(row/img.shape[0]))\n",
    "                        switch=True\n",
    "                if (row==img.shape[0]-1 and switch==False):\n",
    "                    y_list.append(None)\n",
    "        y_list_expanded = []\n",
    "        for y_index in range(len(y_list)):\n",
    "            if (y_list[y_index]==None):\n",
    "                index_adder=1\n",
    "                if (y_index+index_adder >= len(y_list)):\n",
    "                    y_list[y_index]=y_list[y_index-1]\n",
    "                else:\n",
    "                    next_y=y_list[y_index+index_adder]\n",
    "                    while next_y==None:\n",
    "                        index_adder+=1\n",
    "                        if (y_index+index_adder >= len(y_list)):\n",
    "                            if (y_index==0):\n",
    "                                raise ValueError('There are no values in this ECG!')\n",
    "                            else:\n",
    "                                next_y=y_list[y_index-1]\n",
    "                        else:\n",
    "                           next_y=y_list[y_index+index_adder]\n",
    "                    if(y_index==0 or y_index+index_adder >= len(y_list)):\n",
    "                        y_list[y_index]=next_y\n",
    "                    else:\n",
    "                        y_list[y_index]=y_list[y_index-1]+((next_y-y_list[y_index-1])/(index_adder+1))\n",
    "        for y_index in range(len(y_list)):\n",
    "            y_list_expanded.append(y_list[y_index])\n",
    "            if (y_index==len(y_list)-1):\n",
    "                y_list_expanded.append(y_list[y_index])\n",
    "            else:\n",
    "                generated_y=y_list[y_index]+((y_list[y_index+1]-y_list[y_index])/2)\n",
    "                y_list_expanded.append(generated_y)\n",
    "        y_list_expanded=np.interp(y_list_expanded, (0, +1), (-5, +6)) #Rescales data to better reflect the data the Oxford model trained on\n",
    "        return [y_list_expanded,file]\n",
    "    except ValueError as e:\n",
    "        print('Blank image in batch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below loads an image, rescales it to the target size and compresses it to greyscale. It returns a numpy array array for the greyscale image.\n",
    "\n",
    "The name of the ECG file is passed as both input and output arguments. This is a simple way of keeping track of the images during multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(ecg_list):\n",
    "    file=ecg_list[0]\n",
    "    image_y=ecg_list[1]\n",
    "    ecg_seconds=ecg_list[2]\n",
    "    x_width=image_y*ecg_seconds\n",
    "    try:\n",
    "        x=Image.open(file).convert('L')\n",
    "        # To save passing variables to multithreading processes, I've hard-coded image sizes into function:\n",
    "        x=x.resize((x_width,image_y), Image.ANTIALIAS)\n",
    "        x=np.asarray(x,dtype=np.float32)/255\n",
    "        return [x,file]\n",
    "    except SyntaxError as e:\n",
    "        print('Failed to load image: ', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is part of the workflow of preprocessing image files into input vectors for the machine learning model. It exists as an independent function simply to capitalise on multiprocessing for computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_set(ecg_list):\n",
    "    pool = ThreadPool(4) \n",
    "    results = pool.map(load_images, ecg_list)\n",
    "    image_list=[]\n",
    "    file_list=[]\n",
    "    for i in range(len(results)):\n",
    "        image_list.append(results[i][0])\n",
    "        file_list.append(results[i][1])\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    del results\n",
    "    return [image_list,file_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes image files, preprocesses them and returns 'extrapolated' signal data as feature vectors for the machine learning model. The name of the function reflects the fact that the 'bottom layer' of this process is formed of rule-based computing techniques, where the 'top layer' is machine learning-based. The preprocessing that connects the two is, hence, the 'middle layer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_middle_layer(ecg_list,image_y,ecg_seconds,FS):\n",
    "    creator_list=[]\n",
    "    for ecg in ecg_list:\n",
    "        temp_list=[ecg,image_y,ecg_seconds]\n",
    "        creator_list.append(temp_list)\n",
    "        del temp_list\n",
    "    images,files=create_training_set(creator_list)\n",
    "    del creator_list\n",
    "    data=[]\n",
    "    for i in range(len(images)):\n",
    "        mini_data=[files[i],images[i]]\n",
    "        data.append(mini_data)\n",
    "    pool = ThreadPool(4) \n",
    "    results = pool.map(manually_read_image, data)\n",
    "    del data\n",
    "    pool.close() \n",
    "    pool.join()\n",
    "    X=np.zeros((len(results),FS*ecg_seconds))\n",
    "    ecgs=[]\n",
    "    for i in range(len(results)):\n",
    "        X[i,:]=results[i][0]\n",
    "        ecgs.append(results[i][1])\n",
    "    del results\n",
    "    return X,ecgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below simply saves the data returned from the create_middle_layer function to avoid the need to reprocess all the images each time the script is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_array(ecg_list,image_y,ecg_seconds,FS,ground_truth,batch_number):\n",
    "    print('Creating training data...')\n",
    "    X,ecgs=create_middle_layer(ecg_list,image_y,ecg_seconds,FS)\n",
    "    Y=np.zeros((X.shape[0],4))\n",
    "    classes = ['A', 'N', 'O','~']\n",
    "    new_ecg_list=[]\n",
    "    for i in range(X.shape[0]):\n",
    "        index=ecg_list.index(ecgs[i])\n",
    "        y=label_to_one_hot(ground_truth[index])\n",
    "        Y[i,:]=y\n",
    "        new_ecg_list.append(ecgs[i])\n",
    "    length=X.shape[0]\n",
    "    print('Saving ',length,' training examples...')\n",
    "    np.save('training_nps/x_train_'+str(batch_number)+'.npy', X)\n",
    "    np.save('training_nps/y_train_'+str(batch_number)+'.npy', Y)\n",
    "    with open('training_nps/original_record_'+str(batch_number)+'.p', 'wb') as list_file:\n",
    "        pickle.dump(new_ecg_list, list_file)\n",
    "    print('Done!')\n",
    "    del X\n",
    "    del Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below divides training data into batches. This experiment was original run on a machine with 8GB RAM, so a few workarounds like this were required to operate within hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_batches(ecg_list,image_y,ecg_seconds,FS,ground_truth,x,original_ecg_list):\n",
    "    start=0\n",
    "    for batch_number in range(x):\n",
    "        if(os.path.isfile('training_nps/x_train_'+str(batch_number)+'.npy') is False or\n",
    "           os.path.isfile('training_nps/Y_train_'+str(batch_number)+'.npy') is False or\n",
    "           os.path.isfile('training_nps/original_record_'+str(batch_number)+'.p') is False):\n",
    "            print('Generating batches from number',start,'of',x)\n",
    "            break\n",
    "        else:\n",
    "            start=batch_number+1\n",
    "    if (start == x):\n",
    "        return\n",
    "    else:\n",
    "        slice_size=len(original_ecg_list)//x\n",
    "        original_limiter_list=[] #list of the original Physionet record sitting at the start of each slice\n",
    "        image_limiter_list=[] #(k+1)list of indices of the image at the start of each slice, with len(ecg_list) appended\n",
    "        #Create original_limiter_list:\n",
    "        for i in range(x):\n",
    "            original_limiter_list.append(original_ecg_list[i*slice_size])\n",
    "        #Create image_limiter_list:\n",
    "        split_str=ecg_list[0].split('.')[1]\n",
    "        split_str=split_str.split('/')[-1]\n",
    "        split_str=split_str.split('_')[0]\n",
    "        ecg_name=split_str\n",
    "        last_index=0\n",
    "        for ecg_index in range(len(ecg_list)):\n",
    "            split_str=ecg_list[ecg_index].split('.')[1]\n",
    "            split_str=split_str.split('/')[-1]\n",
    "            split_str=split_str.split('_')[0]\n",
    "            if (split_str != ecg_name): #Triggers every time an image belonging to a new ECG record is reached.\n",
    "                if (split_str in original_limiter_list):\n",
    "                    image_limiter_list.append(last_index) #If the new record is a slice limiter, add index to list.\n",
    "                    last_index=ecg_index\n",
    "                ecg_name=split_str #Otherwise, reset ecg_name so all images belonging to this record are ignored.\n",
    "            if (ecg_index==len(ecg_list)-1):\n",
    "                image_limiter_list.append(last_index)\n",
    "                image_limiter_list.append(len(ecg_list)) #Append the length of ecg_list to the list.\n",
    "        print('Image_limiter_list length: ',len(image_limiter_list),' Contents: ',image_limiter_list) #For debugging\n",
    "        #Create and save data in batches:\n",
    "        for i in range(start,x):\n",
    "            start=image_limiter_list[i]\n",
    "            end=image_limiter_list[i+1]\n",
    "            print('Creating and saving batch ',i+1,'of ',x,'...')\n",
    "            save_training_array(ecg_list[start:end],image_y,ecg_seconds,FS,ground_truth[start:end],i)\n",
    "            print('Done with batch ',i+1,'!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is heavily skewed, F1 scoring is much more important than accuracy. The function below calculates the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prediction,truth,F1_list):\n",
    "    F1_list[truth]+=1\n",
    "    F1_list[prediction+4]+=1\n",
    "    if (prediction==truth):\n",
    "        F1_list[prediction+8]+=1\n",
    "    return F1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains two helper functions to translate one-hot vectors (returned by the machine learning model) back into human-readable labels and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_label(one_hot):\n",
    "    y=np.argmax(one_hot)\n",
    "    label=''\n",
    "    if(y==0):\n",
    "        label='AF'\n",
    "    elif(y==1):\n",
    "        label='NSR'\n",
    "    elif(y==2):\n",
    "        label='Other'\n",
    "    elif(y==3):\n",
    "        label='Noisy'\n",
    "    return label\n",
    "\n",
    "def label_to_one_hot(label):\n",
    "    one_hot=np.zeros((1,4))\n",
    "    if(label=='A'):\n",
    "        one_hot[0,0]=1\n",
    "    elif(label=='N'):\n",
    "        one_hot[0,1]=1\n",
    "    elif(label=='O'):\n",
    "        one_hot[0,2]=1\n",
    "    elif(label=='~'):\n",
    "        one_hot[0,3]=1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for calculating F1 score\n",
    "def quick_f1_function(start,F1_list):\n",
    "    if (F1_list[start]+F1_list[start+4]==0 or F1_list[start+8]==0):\n",
    "        f1=0\n",
    "    else:\n",
    "        f1=(2*F1_list[start+8])/(F1_list[start]+F1_list[start+4])\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below makes predictions using a trained model and returns the F1 score for the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model,X,Y,new_ecg_list,batch_size):\n",
    "    F1_list=[0,0,0,0,0,0,0,0,0,0,0,0] #A N O P a n o p Aa Nn Oo Pp\n",
    "    y_pred_per_image = model.predict(X,batch_size=batch_size)\n",
    "    total_predictions=y_pred_per_image.shape[0]\n",
    "    del X\n",
    "    #Check accuracy\n",
    "    maxes=np.argmax(y_pred_per_image,axis=1)\n",
    "    maxes_Y=np.argmax(Y,axis=1)\n",
    "    comparison=maxes==maxes_Y\n",
    "    accuracy=comparison.sum()/comparison.size\n",
    "    del maxes_Y\n",
    "    del comparison\n",
    "    #Convert predictions to one-hot vectors:\n",
    "    zeros=np.zeros((y_pred_per_image.shape[0],y_pred_per_image.shape[1]))\n",
    "    zeros[np.arange(y_pred_per_image.shape[0]), maxes] = 1\n",
    "    y_pred_per_image=zeros\n",
    "    del maxes\n",
    "    del zeros\n",
    "    #Create a list of of total_predictions x 2, containing start and stop indices for each unique record:\n",
    "    ecgs_from_same_record=[]\n",
    "    split_str=new_ecg_list[0].split('.')[1]\n",
    "    split_str=split_str.split('/')[-1]\n",
    "    split_str=split_str.split('_')[0]\n",
    "    ecg_name=split_str\n",
    "    last_index=0\n",
    "    for ecg_index in range(len(new_ecg_list)):\n",
    "        split_str=new_ecg_list[ecg_index].split('.')[1]\n",
    "        split_str=split_str.split('/')[-1]\n",
    "        split_str=split_str.split('_')[0]\n",
    "        if (split_str!=ecg_name):\n",
    "            start_stop_index=[]\n",
    "            start_stop_index.append(last_index)\n",
    "            start_stop_index.append(ecg_index)\n",
    "            last_index=ecg_index\n",
    "            ecg_name=split_str\n",
    "            ecgs_from_same_record.append(start_stop_index)\n",
    "        if (ecg_index==len(new_ecg_list)-1):\n",
    "            start_stop_index=[]\n",
    "            start_stop_index.append(last_index)\n",
    "            start_stop_index.append(len(new_ecg_list))\n",
    "            ecgs_from_same_record.append(start_stop_index)\n",
    "    y_pred=np.zeros((len(ecgs_from_same_record),4))\n",
    "    y_true=np.zeros((len(ecgs_from_same_record),4))\n",
    "    for i in range(len(ecgs_from_same_record)):\n",
    "        a=ecgs_from_same_record[i][0]\n",
    "        b=ecgs_from_same_record[i][1]\n",
    "        y_pred[i,:]=np.sum(y_pred_per_image[a:b,:], axis=0)\n",
    "        y_true[i,:]=Y[a,:]\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        this_y=np.argmax(y_true[i,:])\n",
    "        this_pred=np.argmax(y_pred[i,:])\n",
    "        F1_list=F1_score(this_pred,this_y,F1_list)\n",
    "    f1_a=quick_f1_function(0,F1_list)\n",
    "    f1_n=quick_f1_function(1,F1_list)\n",
    "    f1_o=quick_f1_function(2,F1_list)\n",
    "    f1_total=(f1_a+f1_n+f1_o) / 3\n",
    "    print('Accuracy: ' + str(accuracy) + '%')\n",
    "    print('F1 score: ' + str(f1_total))\n",
    "    return f1_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is a custom callback for Keras model, where only the model weights achieving the best F1 score are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_callback(keras.callbacks.Callback):\n",
    "    def __init__(self,X_val,Y_val,new_ecg_list,k,patience,batch_size):\n",
    "        self.best_f1=0\n",
    "        self.x_val=X_val\n",
    "        self.y_val=Y_val\n",
    "        self.best_epoch=0\n",
    "        self.k=k\n",
    "        self.new_ecg_list=new_ecg_list\n",
    "        self.patience=patience\n",
    "        self.batch_size=batch_size\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        this_f1=make_predictions(self.model,self.x_val,self.y_val,self.new_ecg_list,self.batch_size)\n",
    "        if (this_f1>self.best_f1):\n",
    "            self.best_f1=this_f1\n",
    "            print('F1 score improved to ',self.best_f1,', saving model...')\n",
    "            self.model.save_weights('OxResNet_30s_run' + str(self.k) + '.h5')\n",
    "            self.best_epoch=epoch\n",
    "            print('Model saved!')\n",
    "        else:\n",
    "            print('F1 score was ', this_f1,'. Did not improve from ', self.best_f1)\n",
    "            if (epoch>=self.best_epoch+self.patience):\n",
    "                print('That is',self.patience,'epochs with no improvement. Stopping training.')\n",
    "                self.model.stop_training = True\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below trains the Keras model using weighted loss to reflect the skewed data distribution and a custom callback (see above) to only save the weights that achieve the best F1 score.\n",
    "\n",
    "Note the class weights are specific to the Physionet AF Challenge dataset. They will need manually adjusting if using this script with an alternative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_batch(model,X_train,Y_train, X_val, Y_val, new_ecg_list,k,batch_size,epochs,patience):\n",
    "    weights={} \n",
    "    for i in range(Y_train.shape[1]):\n",
    "        weight_array=np.zeros(Y_train.shape[1])\n",
    "        weight_array[i]=1\n",
    "        this_weight=(Y_train==weight_array).all(axis=1).sum()/Y_train.shape[0]\n",
    "        weights.update({i:this_weight})\n",
    "    f1_callback = F1_callback(X_val,Y_val,new_ecg_list,k,patience,batch_size)\n",
    "    model.fit(x=X_train, \n",
    "              y=Y_train, \n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              verbose=1,\n",
    "              callbacks=[f1_callback],\n",
    "              class_weight=weights)\n",
    "    del X_train\n",
    "    del Y_train\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below loads and pre-processes the data for each batch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(ecg_list,image_y,ecg_seconds,FS,ground_truth,K,batch_size,epochs,patience,\n",
    "                  classes,data_split,big_data,visualise_data_each_cycle,dropout,use_adam_optimiser,lr):\n",
    "    \n",
    "    data_segments_per_k=int(data_split//K)\n",
    "    f1_total=0\n",
    "    optimiser=Adam(lr=lr)\n",
    "    if (use_adam_optimiser==False):\n",
    "        optimiser=SGD(lr=lr)\n",
    "    #No need to batch up the data if it will all fit into RAM:\n",
    "    if (big_data==False):\n",
    "        for k in range(K):\n",
    "            model = ResNet_model_low_dropout(ecg_seconds*FS,classes)\n",
    "            if(os.path.isfile('OxResNet_30s_run' + str(k) + '.h5')==False):\n",
    "                model.compile(optimizer='adam',\n",
    "                              loss='categorical_crossentropy',\n",
    "                              metrics=['accuracy'])\n",
    "                model.summary()\n",
    "                X_train=np.zeros((1,1))\n",
    "                Y_train=np.zeros((1,1))\n",
    "                X_val=np.zeros((1,1))\n",
    "                Y_val=np.zeros((1,1))\n",
    "                new_ecg_list=[]\n",
    "                first_batch_loaded=False\n",
    "                for i in range(0, K*data_segments_per_k, data_segments_per_k):\n",
    "                    if (i==k*data_segments_per_k):\n",
    "                        X_val=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                        Y_val=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                        with open ('training_nps/original_record_'+str(i)+'.p', 'rb') as list_file:\n",
    "                            new_ecg_list = pickle.load(list_file)\n",
    "                        for z in range(i+1,i+data_segments_per_k):\n",
    "                            X_val=np.concatenate((X_val,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                            Y_val=np.concatenate((Y_val,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                            with open ('training_nps/original_record_'+str(z)+'.p', 'rb') as list_file:\n",
    "                                tmp_ecg_list = pickle.load(list_file)\n",
    "                                new_ecg_list=new_ecg_list+tmp_ecg_list\n",
    "                                del tmp_ecg_list\n",
    "                    else:\n",
    "                        if (first_batch_loaded==False):\n",
    "                            X_train=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                            Y_train=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                            for z in range(i+1,i+data_segments_per_k):\n",
    "                                X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                                Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                            first_batch_loaded=True\n",
    "                        else:\n",
    "                            X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(i)+'.npy')),axis=0)\n",
    "                            Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(i)+'.npy')),axis=0)\n",
    "                            for z in range(i+1,i+data_segments_per_k):\n",
    "                                X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                                Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                if (visualise_data_each_cycle):\n",
    "                    visualise_data(X_val,Y_val,new_ecg_list)\n",
    "                X_train=np.expand_dims(X_train, axis=-1)\n",
    "                X_val=np.expand_dims(X_val, axis=-1)\n",
    "                print('Cycle:', k, 'of',K,'X_train:', X_train.shape, 'Y_train:', Y_train.shape, 'X_val:', X_val.shape, 'Y_val:', Y_val.shape)\n",
    "                model=run_training_batch(model,X_train,Y_train,X_val,Y_val,new_ecg_list,k,batch_size,epochs,patience)\n",
    "                print('Cycle ' + str(k+1) + ' of ' + str(K) + ' complete')\n",
    "                del X_train\n",
    "                del Y_train\n",
    "                del X_val\n",
    "                del Y_val\n",
    "        \n",
    "        for k in range(K):\n",
    "            model = ResNet_model_custom_dropout(ecg_seconds*FS,classes,dropout)\n",
    "            model=load_old_model(model,'OxResNet_30s_run' + str(k) + '.h5')\n",
    "            sgd=SGD(lr=0.0001, decay=0, momentum=0.9, nesterov=True)\n",
    "            model.compile(loss='categorical_crossentropy', \n",
    "                        optimizer=optimiser, \n",
    "                        metrics=['acc'])\n",
    "            model.summary()\n",
    "            X_train=np.zeros((1,1))\n",
    "            Y_train=np.zeros((1,1))\n",
    "            X_val=np.zeros((1,1))\n",
    "            Y_val=np.zeros((1,1))\n",
    "            new_ecg_list=[]\n",
    "            first_batch_loaded=False\n",
    "            for i in range(0, K*data_segments_per_k, data_segments_per_k):\n",
    "                if (i==k*data_segments_per_k):\n",
    "                    X_val=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                    Y_val=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                    with open ('training_nps/original_record_'+str(i)+'.p', 'rb') as list_file:\n",
    "                        new_ecg_list = pickle.load(list_file)\n",
    "                    for z in range(i+1,i+data_segments_per_k):\n",
    "                        X_val=np.concatenate((X_val,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                        Y_val=np.concatenate((Y_val,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                        with open ('training_nps/original_record_'+str(z)+'.p', 'rb') as list_file:\n",
    "                            tmp_ecg_list = pickle.load(list_file)\n",
    "                            new_ecg_list=new_ecg_list+tmp_ecg_list\n",
    "                            del tmp_ecg_list\n",
    "                else:\n",
    "                    if (first_batch_loaded==False):\n",
    "                        X_train=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                        Y_train=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                        for z in range(i+1,i+data_segments_per_k):\n",
    "                            X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                            Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                        first_batch_loaded=True\n",
    "                    else:\n",
    "                        X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(i)+'.npy')),axis=0)\n",
    "                        Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(i)+'.npy')),axis=0)\n",
    "                        for z in range(i+1,i+data_segments_per_k):\n",
    "                            X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                            Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "            if (visualise_data_each_cycle):\n",
    "                visualise_data(X_val,Y_val,new_ecg_list)\n",
    "            X_train=np.expand_dims(X_train, axis=-1)\n",
    "            X_val=np.expand_dims(X_val, axis=-1)\n",
    "            print('Cycle:', k, 'of',K,'X_train:', X_train.shape, 'Y_train:', Y_train.shape, 'X_val:', X_val.shape, 'Y_val:', Y_val.shape)\n",
    "            model=run_training_batch(model,X_train,Y_train,X_val,Y_val,new_ecg_list,k,batch_size,epochs,patience)\n",
    "            model=load_model('OxResNet_5s_run' + str(k) + '.h5')\n",
    "            f1_total+=make_predictions(model,X_val,Y_val,new_ecg_list,batch_size)\n",
    "            print('Cycle ' + str(k+1) + ' of ' + str(K) + ' complete')\n",
    "            del X_train\n",
    "            del Y_train\n",
    "            del X_val\n",
    "            del Y_val\n",
    "            del new_ecg_list\n",
    "    \n",
    "    #If there is too much data, cannot load into RAM all in one go:\n",
    "    \n",
    "    #(Could use a generator, but I think it's as well to just train initially on one half of the data with ADAM optimiser\n",
    "    #and re-train on the other half of the data with SGD + Nesterov & lr=0.001 for fine-tuning.)\n",
    "    elif (big_data==True):\n",
    "        try:\n",
    "            training_runs_array = np.load('training_runs.npy')\n",
    "        except FileNotFoundError:\n",
    "            training_runs_array=np.zeros(1)\n",
    "            np.save('training_runs.npy',training_runs_array)\n",
    "        k=int(training_runs_array[0])\n",
    "        \n",
    "        #First round of training freezes base model and just trains new dense layers:\n",
    "        if(os.path.isfile('OxResNet_30s_run' + str(k) + '.h5')==False):\n",
    "            model = ResNet_model_low_dropout(ecg_seconds*FS,classes)\n",
    "            model.compile(optimizer='adam',\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "            model.summary()\n",
    "            X_train=np.zeros((1,1))\n",
    "            Y_train=np.zeros((1,1))\n",
    "            X_val=np.zeros((1,1))\n",
    "            Y_val=np.zeros((1,1))\n",
    "            new_ecg_list=[]\n",
    "            first_batch_loaded=False\n",
    "            for i in range(0, K*data_segments_per_k, data_segments_per_k):\n",
    "                if (i==k*data_segments_per_k):\n",
    "                    X_val=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                    Y_val=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                    with open ('training_nps/original_record_'+str(i)+'.p', 'rb') as list_file:\n",
    "                        new_ecg_list = pickle.load(list_file)\n",
    "                    for z in range(i+2,i+data_segments_per_k-1,2):\n",
    "                        X_val=np.concatenate((X_val,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                        Y_val=np.concatenate((Y_val,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                        with open ('training_nps/original_record_'+str(z)+'.p', 'rb') as list_file:\n",
    "                            tmp_ecg_list = pickle.load(list_file)\n",
    "                            new_ecg_list=new_ecg_list+tmp_ecg_list\n",
    "                            del tmp_ecg_list\n",
    "                else:\n",
    "                    if (first_batch_loaded==False):\n",
    "                        X_train=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                        Y_train=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                        for z in range(i+2,i+data_segments_per_k,2):\n",
    "                            X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                            Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                        first_batch_loaded=True\n",
    "                    else:\n",
    "                        X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(i)+'.npy')),axis=0)\n",
    "                        Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(i)+'.npy')),axis=0)\n",
    "                        for z in range(i+2,i+data_segments_per_k,2):\n",
    "                            X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                            Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "            if (visualise_data_each_cycle):\n",
    "                visualise_data(X_val,Y_val,new_ecg_list)\n",
    "            X_train=np.expand_dims(X_train, axis=-1)\n",
    "            X_val=np.expand_dims(X_val, axis=-1)\n",
    "            print('Cycle:', k, 'of',K,'X_train:', X_train.shape, 'Y_train:', Y_train.shape, 'X_val:', X_val.shape, 'Y_val:', Y_val.shape)\n",
    "            model=run_training_batch(model,X_train,Y_train,X_val,Y_val,new_ecg_list,k,batch_size,epochs,patience)\n",
    "            print('Cycle ' + str(k+1) + ' of ' + str(K) + ' complete')\n",
    "            k+=1\n",
    "            if (k<K):\n",
    "                training_runs_array[0]=k\n",
    "                np.save('training_runs.npy',training_runs_array)\n",
    "                restartkernel()\n",
    "            else:\n",
    "                k=0\n",
    "                training_runs_array[0]=k\n",
    "                np.save('training_runs.npy',training_runs_array)\n",
    "                restartkernel()\n",
    "        \n",
    "        #After initial training, base model is unfrozen and the whole model is retrained:\n",
    "        try:\n",
    "            f1_results_array = np.load('f1_results_array.npy')\n",
    "        except FileNotFoundError:\n",
    "            f1_results_array=np.zeros(K)\n",
    "            np.save('f1_results_array.npy',f1_results_array)\n",
    "        model = ResNet_model_custom_dropout(ecg_seconds*FS,classes,dropout)\n",
    "        model=load_old_model(model,'OxResNet_30s_run' + str(k) + '.h5')\n",
    "        model.compile(optimizer=optimiser,\n",
    "                          loss='categorical_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        X_train=np.zeros((1,1))\n",
    "        Y_train=np.zeros((1,1))\n",
    "        X_val=np.zeros((1,1))\n",
    "        Y_val=np.zeros((1,1))\n",
    "        new_ecg_list=[]\n",
    "        first_batch_loaded=False\n",
    "        for i in range(1, K*data_segments_per_k, data_segments_per_k):\n",
    "            if (i==k*data_segments_per_k+1):\n",
    "                X_val=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                Y_val=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                with open ('training_nps/original_record_'+str(i)+'.p', 'rb') as list_file:\n",
    "                    new_ecg_list = pickle.load(list_file)\n",
    "                for z in range(i+2,i+data_segments_per_k-1,2):\n",
    "                    X_val=np.concatenate((X_val,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                    Y_val=np.concatenate((Y_val,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                    with open ('training_nps/original_record_'+str(z)+'.p', 'rb') as list_file:\n",
    "                        tmp_ecg_list = pickle.load(list_file)\n",
    "                        new_ecg_list=new_ecg_list+tmp_ecg_list\n",
    "                        del tmp_ecg_list\n",
    "            else:\n",
    "                if (first_batch_loaded==False):\n",
    "                    X_train=np.load('training_nps/x_train_'+str(i)+'.npy')\n",
    "                    Y_train=np.load('training_nps/y_train_'+str(i)+'.npy')\n",
    "                    for z in range(i,i+data_segments_per_k-1,2):\n",
    "                        X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                        Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                    first_batch_loaded=True\n",
    "                else:\n",
    "                    X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(i)+'.npy')),axis=0)\n",
    "                    Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(i)+'.npy')),axis=0)\n",
    "                    for z in range(i+2,i+data_segments_per_k-1,2):\n",
    "                        X_train=np.concatenate((X_train,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                        Y_train=np.concatenate((Y_train,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "        if (visualise_data_each_cycle):\n",
    "            visualise_data(X_val,Y_val,new_ecg_list)\n",
    "        X_train=np.expand_dims(X_train, axis=-1)\n",
    "        X_val=np.expand_dims(X_val, axis=-1)\n",
    "        print('Cycle:', k+1, 'of',K,'X_train:', X_train.shape, 'Y_train:', Y_train.shape, 'X_val:', X_val.shape, 'Y_val:', Y_val.shape)\n",
    "        model=run_training_batch(model,X_train,Y_train,X_val,Y_val,new_ecg_list,k,batch_size,epochs,patience)\n",
    "        model.load_weights('OxResNet_30s_run' + str(k) + '.h5')\n",
    "\n",
    "        del X_train\n",
    "        del Y_train\n",
    "\n",
    "        #Load full validation data:\n",
    "        X_val=np.load('training_nps/x_train_'+str(k)+'.npy')\n",
    "        Y_val=np.load('training_nps/y_train_'+str(k)+'.npy')\n",
    "        with open ('training_nps/original_record_'+str(k)+'.p', 'rb') as list_file:\n",
    "            new_ecg_list = pickle.load(list_file)\n",
    "        for z in range(k+1,k+data_segments_per_k):\n",
    "                X_val=np.concatenate((X_val,np.load('training_nps/x_train_'+str(z)+'.npy')),axis=0)\n",
    "                Y_val=np.concatenate((Y_val,np.load('training_nps/y_train_'+str(z)+'.npy')),axis=0)\n",
    "                with open ('training_nps/original_record_'+str(z)+'.p', 'rb') as list_file:\n",
    "                    tmp_ecg_list = pickle.load(list_file)\n",
    "                    new_ecg_list=new_ecg_list+tmp_ecg_list\n",
    "        X_val=np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "\n",
    "        f1_total=make_predictions(model,X_val,Y_val,new_ecg_list,batch_size)\n",
    "        print('Cycle ' + str(k+1) + ' of ' + str(K) + ' complete')\n",
    "        \n",
    "        f1_results_array[k]=f1_total\n",
    "        k+=1\n",
    "        training_runs_array[0]=k\n",
    "        np.save('f1_results_array.npy',f1_results_array)\n",
    "        np.save('training_runs.npy',training_runs_array)\n",
    "        if (k<K):\n",
    "            restartkernel() \n",
    "        else:\n",
    "            del X_val\n",
    "            del Y_val\n",
    "            del new_ecg_list\n",
    "            del model\n",
    "            \n",
    "    #Final lines are common to all above funcitons (i.e. regardless of big_data variable):\n",
    "    f1_total=sum(f1_results_array)/K\n",
    "    print('Completed ',K,'-fold cross-validation. F1 score: ',f1_total)\n",
    "    text_file = open(\"F1_score.txt\", \"w\")\n",
    "    text_file.write(\"F1_score after \"+ str(k) +\"-fold validation: \" + \"{:.2f}\".format(f1_total))\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below produces a random sample of five ECGs in human-readable forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_data(X,Y,new_ecg_list):\n",
    "    for i in range(5):\n",
    "        rand=randint(0,X.shape[0])\n",
    "        fig, ax = plt.subplots()\n",
    "        split_str=new_ecg_list[rand].split('.')[1]\n",
    "        split_str=split_str.split('/')[-1]\n",
    "        split_str=split_str.split('_')[0]\n",
    "        title=split_str + ': ' + one_hot_to_label(Y[rand,:])\n",
    "        ax.plot(X[rand,0:1000],'k')\n",
    "        ax.set_title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_model_custom_dropout(WINDOW_SIZE,OUTPUT_CLASS,dropout):\n",
    "    # Add CNN layers left branch (higher frequencies)\n",
    "    # Parameters from paper\n",
    "    resnet_model=load_model('ResNet_30s_34lay_16conv.hdf5')\n",
    "    for l in resnet_model.layers:\n",
    "        l.trainable=False\n",
    "    resnet_model.layers.pop()\n",
    "    inp = resnet_model.input\n",
    "    out =resnet_model.layers[-1].output\n",
    "    model2 = Model(inp, out)\n",
    "    model=Sequential()\n",
    "    model.add(model2)\n",
    "    model.add(Dense(512,activation='relu',name='new_layer_1'))\n",
    "    model.add(Dropout(dropout,name='new_layer_2'))\n",
    "    model.add(Dense(512,activation='relu',name='new_layer_3'))\n",
    "    model.add(Dropout(dropout,name='new_layer_4'))\n",
    "    model.add(Dense(4,activation='softmax',name='new_layer_5'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions take the pretrained model from the University of Oxford and tweak the architecture a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet_model_low_dropout(WINDOW_SIZE,OUTPUT_CLASS):\n",
    "    # Add CNN layers left branch (higher frequencies)\n",
    "    # Parameters from paper\n",
    "    resnet_model=load_model('ResNet_30s_34lay_16conv.hdf5')\n",
    "    for l in resnet_model.layers:\n",
    "        l.trainable=False\n",
    "    resnet_model.layers.pop()\n",
    "    inp = resnet_model.input\n",
    "    out =resnet_model.layers[-1].output\n",
    "    model2 = Model(inp, out)\n",
    "    model=Sequential()\n",
    "    model.add(model2)\n",
    "    model.add(Dense(512,activation='relu',name='new_layer_1'))\n",
    "    model.add(Dropout(0,name='new_layer_2'))\n",
    "    model.add(Dense(512,activation='relu',name='new_layer_3'))\n",
    "    model.add(Dropout(0,name='new_layer_4'))\n",
    "    model.add(Dense(4,activation='softmax',name='new_layer_5'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_old_model(old_model,weights_path):\n",
    "    try:\n",
    "        old_model.load_weights(weights_path)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    resnet_model=load_model('ResNet_30s_34lay_16conv.hdf5')\n",
    "    resnet_model.layers.pop()\n",
    "    inp = resnet_model.input\n",
    "    out =resnet_model.layers[-1].output\n",
    "    model2 = Model(inp, out)\n",
    "    model=Sequential()\n",
    "    model.add(model2)\n",
    "    for l in old_model.layers[-5:]:\n",
    "        model.add(l)\n",
    "    try:\n",
    "        model.load_weights(weights_path)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is an inelegant workaround to dump the cache. See the following function for clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restartkernel() :\n",
    "    from IPython.display import display_html\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart();restartTime = 5000;setTimeout(function(){ Jupyter.notebook.execute_all_cells(); }, restartTime);</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a real faff, but due to a memory leak in MatPlotLib, the RAM clogged up when trying to create and write a large number of images to disk. Restarting the iPython notebook dumped the memory and sorts this out.\n",
    "\n",
    "The number of cycles is written to disk and loaded up every time the script is restarted. In this particular iteration of the script, the machine used during our study can handle batches of 250 records (bearing in mind each record = approx 10-20 ECG images) and we're dealing with just under 10,000 ECG records, so 10,000/250 = 40 cycles.\n",
    "\n",
    "When we have run this code on newer machines as a straight Python script (i.e. outside Jupyter), the issue has not occurred. To do this, simply save this script as .py file and replace the restartkernel() function thus:\n",
    "```python\n",
    "def restartkernel():\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_main(image_y,ecg_seconds,FS,directory,sliding_window_seconds,max_seconds_of_padding):   \n",
    "    records_per_batch=250\n",
    "    total_cycles=40\n",
    "    \n",
    "    try:\n",
    "        script_runs_array = np.load('script_runs.npy')\n",
    "    except FileNotFoundError:\n",
    "        script_runs_array=np.zeros(1)\n",
    "        np.save('script_runs.npy',script_runs_array)\n",
    "    script_runs=int(script_runs_array[0])\n",
    "    \n",
    "    if (script_runs<total_cycles):\n",
    "        print('Running batch',script_runs+1,'of,',total_cycles)\n",
    "        [ecg_list,ground_truth,original_ecg_list]=create_ecg_images(image_y,ecg_seconds,FS,'training2017',sliding_window_seconds,max_seconds_of_padding,script_runs,records_per_batch)\n",
    "        script_runs+=1\n",
    "        script_runs_array[0]=script_runs\n",
    "        np.save('script_runs.npy',script_runs_array)\n",
    "        restartkernel()\n",
    "    else:\n",
    "         [ecg_list,ground_truth,original_ecg_list]=load_lists()\n",
    "            \n",
    "    return [ecg_list,ground_truth,original_ecg_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below executes the functions in order. Adjustable variables can be set here if the code is being used with another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    image_y=150 # height of generated ECG images in px, see manually_read_image function for explanation of why 150 chosen\n",
    "    ecg_seconds=30 # set at 30 to capitalise on pre-trained model from Oxford paper\n",
    "    FS=300 # frames / second (ECG resolution in Hz within source data)\n",
    "    data_split=20 # to ensure the data is processed in chunks that fit in RAM (20 works on my machine)\n",
    "    big_data=True # if processed data-set (i.e. number of images x (FS * ecg_seconds)-dimensional vectors) too big for RAM\n",
    "    k=5 # for k-fold validation\n",
    "    epochs=1000 # for ML model\n",
    "    batch_size=64 # for ML model\n",
    "    patience=10 # for ML model\n",
    "    classes=4 # for ML model\n",
    "    sliding_window_seconds=1 # create more training examples if records are > ecg_seconds\n",
    "    max_seconds_of_padding=10 # pad records so model can cope with records < ecg_seconds\n",
    "    F1_list=np.zeros((1,12)) # corresponding to: A N O P a n o p Aa Nn Oo Pp\n",
    "    #(where A=AF, N=NSR, O=other, P=too noisy to interpret. Criteria from Physionet 2017 AF Challenge.)\n",
    "    visualise_data=False # prints a selection of data before each training cycle\n",
    "    \n",
    "    #Note: ML model will always run once with a frozen base model and Adam optimiser with dropout=0 for (new) dense\n",
    "    #layers (2 x 512 with ReLU -> output with )\n",
    "    dropout=0.3 # for final, dense layers of model\n",
    "    use_adam_optimiser=True # if false will use SGD with Nesterov\n",
    "    lr=0.001 # learning rate\n",
    "    \n",
    "    #Creates lists of images, labels and original ECGs:\n",
    "    ecg_list,ground_truth,original_ecg_list=prep_main(image_y,ecg_seconds,FS,classes,\n",
    "                                                      sliding_window_seconds,max_seconds_of_padding)\n",
    "    \n",
    "    #Generates, imports and processes images in batches, saves resulting ndarrays to disk:\n",
    "    create_x_batches(ecg_list,image_y,ecg_seconds,FS,ground_truth,data_split,original_ecg_list)\n",
    "    \n",
    "    #Creates and trains the model then prints F1 score:\n",
    "    retrain_model(ecg_list,image_y,ecg_seconds,FS,ground_truth,k,batch_size,epochs,patience,\n",
    "                  classes,data_split,big_data,visualise_data,dropout,use_adam_optimiser,lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this code is run as a straight Python script (i.e. not in Jupyter), this is the part that will go within the\n",
    "```python\n",
    "if __name__=='__main__':\n",
    "```\n",
    "clause:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
